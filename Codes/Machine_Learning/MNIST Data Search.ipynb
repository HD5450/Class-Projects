{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MNIST Data Search.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"WEEtm_26YVAR","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","from random import choice\n","seed = 1 \n","np.random.seed(seed)\n","\n","from keras.datasets import mnist\n","import matplotlib.pyplot as plt\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.utils import np_utils\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"metadata":{"id":"C9v_aIaUYd9J","colab_type":"code","colab":{}},"cell_type":"code","source":["# Get data\n","(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n","X_train, X_dev, Y_train, Y_dev = train_test_split(X_train, Y_train, test_size=1/6, random_state=seed)\n","\n","# Preprocess data\n","num_of_pixels = X_train.shape[1]*X_train.shape[2]\n","X_train = X_train.reshape(X_train.shape[0], num_of_pixels).astype('float32')\n","X_dev = X_dev.reshape(X_dev.shape[0], num_of_pixels).astype('float32')\n","X_test = X_test.reshape(X_test.shape[0], num_of_pixels).astype('float32')\n","X_train = X_train / 255\n","X_dev = X_dev / 255\n","X_test = X_test / 255\n","\n","Y_train = np_utils.to_categorical(Y_train)\n","Y_dev = np_utils.to_categorical(Y_dev)\n","Y_test = np_utils.to_categorical(Y_test)\n","num_classes = Y_test.shape[1]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5iNfAdVwYizi","colab_type":"code","colab":{}},"cell_type":"code","source":["# create hyperparameter grid\n","\n","batch_size = 20\n","epochs = 10\n","neurons = np.arange(30,151)\n","layers = np.arange(2,5)\n","loss = ['categorical_crossentropy','categorical_hinge']\n","opt = ['sgd','rmsprop','adagrad','adadelta','adam','adamax','nadam']\n","act = ['relu','sigmoid']\n","number_of_models = len(neurons)*len(layers)*len(loss)*len(opt)*len(act)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Qu8W-d7uZOXs","colab_type":"code","colab":{}},"cell_type":"code","source":["pop_size = int(number_of_models / 100)\n","ch = np.random.choice # here for abbrev. purposes\n","pop = [(ch(neurons),ch(layers),ch(loss),ch(opt),ch(act)) for i in range(pop_size)]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Pq_108xYesFh","colab_type":"code","colab":{}},"cell_type":"code","source":["def model_from_hyper(hyperparameters):\n","  neurons,layers,loss,opt,act = hyperparameters\n","  \n","  # first split neurons-num_classes up among layers-1\n","  layers -= 1\n","  neurons -= num_classes - layers # subtract by layers now for zero check later\n","  seperator = [0]*neurons + [1]*(layers-1)\n","  np.random.shuffle(seperator)\n","  neuron_list = [0]*layers\n","  i = 0\n","  for n in seperator:\n","    if not n:\n","      neuron_list[i] += 1\n","    else:\n","      i += 1\n","  neuron_list = np.sort(neuron_list)[::-1]\n","  \n","  # make sure there is at least 1 neuron per layer\n","  model = Sequential()\n","  model.add(Dense(neuron_list[0]+1,input_dim=num_of_pixels,activation=act))\n","  for l in range(1,layers-1):\n","    model.add(Dense(neuron_list[l]+1,activation=act))\n","  model.add(Dense(num_classes, activation='softmax'))\n","  model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])\n","  \n","  return model\n","\n","# here to save time\n","acc_dict = {}\n","\n","# we will use accuracy as our fitness metric\n","def fitness(individual):\n","  # get pre-saved accuracy if available\n","  acc = acc_dict.get(individual)\n","  \n","  if not acc:\n","    model = model_from_hyper(individual)\n","    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n","  \n","    # save accuracy\n","    acc = model.evaluate(X_dev, Y_dev, verbose=0)[1]\n","    acc_dict[individual] = acc\n","  \n","  return acc\n","\n","# child will have the 'average' hyperparameter of parents\n","def crossover(p1,p2):\n","  neurons = int((p1[0]+p2[0])/2)\n","  layers = int((p1[1]+p2[1])/2)\n","  loss = np.random.choice((p1[2],p2[2]))\n","  opt = np.random.choice((p1[3],p2[3]))\n","  act = np.random.choice((p1[4],p2[4]))\n","  \n","  return (neurons, layers, loss, opt, act)\n","\n","# adds 'random' noise to hyperparameters of child population\n","def mutation(population, child_indices, mutation_frac):\n","  child_pop_size = len(child_indices)\n","  np.random.shuffle(child_indices)\n","  \n","  # mutate fraction of children\n","  for i in child_indices[:int(child_pop_size*mutation_frac)]:\n","    new_neurons = int(population[i][0] + np.random.random())\n","    new_layers = int(population[i][1] + np.random.random())\n","    new_loss = population[i][2]\n","    if np.random.random() > 0.5:\n","      new_loss = loss[len(loss) - loss.index(new_loss) - 1]\n","    new_opt = population[i][3]\n","    if np.random.random() > 0.5:\n","      new_opt = opt[len(opt) - opt.index(new_opt) - 1]\n","    new_act = population[i][4]\n","    if np.random.random() > 0.5:\n","      new_act = act[len(act) - act.index(new_act) - 1]\n","    population[i] = (new_neurons,new_layers,new_loss,new_opt,new_act)\n","      \n","  return population\n","\n","# the fun part\n","def GA(population, tournament_size=10, max_generation_num=10,\n","       mutation_frac=0.25, survivers=.75):\n","  for i in range(max_generation_num):\n","    # 90% of the population survives\n","    indices = np.arange(pop_size)\n","    np.random.shuffle(indices)\n","    indices = indices[:int(pop_size*(1-survivers))]\n","    for j in indices:\n","      tournament = [choice(population) for i in range(tournament_size)]\n","      p1 = max(tournament, key=fitness)\n","      \n","      tournament = [choice(population) for i in range(tournament_size)]\n","      p2 = max(tournament, key=fitness)\n","      \n","      population[j] = crossover(p1,p2) #replace dead individual\n","    # only mutate new individuals\n","    population = mutation(population, indices, mutation_frac)\n","  return population\n","\n","king_hyperparameters = max(GA(pop), key=fitness)\n","king = model_from_hyper(king_hyperparameters)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NzkaaUWseslC","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}